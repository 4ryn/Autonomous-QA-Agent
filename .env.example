# ==============================================================================
# AUTONOMOUS QA AGENT - ENVIRONMENT CONFIGURATION
# ==============================================================================
# Copy this file to .env and fill in your actual values
# Never commit .env file to git (it's already in .gitignore)

# Groq API (RECOMMENDED - Free, Fast, No RAM issues)
USE_GROQ=True
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama-3.1-8b-instant
# ==============================================================================
# QDRANT CLOUD CONFIGURATION (Vector Database - API Key Required)
# ==============================================================================
# Sign up for free: https://cloud.qdrant.io/
# Create a cluster and get your credentials

# Your Qdrant Cloud cluster URL (get from dashboard)
# Example: https://a1b2c3d4-e5f6-7890-abcd-ef1234567890.cloud.qdrant.io
QDRANT_URL=https://your-cluster-url.cloud.qdrant.io

# Your Qdrant Cloud API Key (generate in dashboard)
# Example: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
QDRANT_API_KEY=your_qdrant_api_key_here

# Collection name (can keep default)
QDRANT_COLLECTION_NAME=qa_agent_doc

# Use cloud (set to True for Qdrant Cloud)
QDRANT_USE_CLOUD=True

# ==============================================================================
# OPTIONAL: LOCAL QDRANT CONFIGURATION
# ==============================================================================
# If you prefer to run Qdrant locally instead of cloud:
# 1. Set QDRANT_USE_CLOUD=False
# 2. Run: docker run -p 6333:6333 qdrant/qdrant
# 3. Uncomment and use these settings:

# QDRANT_USE_CLOUD=False
# QDRANT_HOST=localhost
# QDRANT_PORT=6333

# ==============================================================================
# EMBEDDING MODEL CONFIGURATION (No API Key Required)
# ==============================================================================
# Sentence Transformers downloads models locally to:
# ~/.cache/torch/sentence_transformers/

# Embedding model (choose one)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Available models:
# - all-MiniLM-L6-v2 (fast, 384 dim, recommended)
# - all-mpnet-base-v2 (better quality, 768 dim, slower)
# - multi-qa-MiniLM-L6-cos-v1 (optimized for Q&A)

# ==============================================================================
# TEXT CHUNKING CONFIGURATION
# ==============================================================================
# Adjust based on your document size and detail needs

# Chunk size in characters
CHUNK_SIZE=500

# Overlap between chunks (helps maintain context)
CHUNK_OVERLAP=50

# Recommended settings:
# - Small docs (< 10 pages): CHUNK_SIZE=300, CHUNK_OVERLAP=30
# - Medium docs (10-50 pages): CHUNK_SIZE=500, CHUNK_OVERLAP=50
# - Large docs (> 50 pages): CHUNK_SIZE=1000, CHUNK_OVERLAP=100

# ==============================================================================
# APPLICATION SETTINGS
# ==============================================================================

# Application title (shown in browser tab)
APP_TITLE=Autonomous QA Agent

# Streamlit port
APP_PORT=8501

# Debug mode (shows detailed logs)
DEBUG_MODE=False

# ==============================================================================
# PERFORMANCE TUNING
# ==============================================================================
# Uncomment and adjust these for performance optimization:

# Maximum tokens for LLM generation
# MAX_TOKENS=2000

# Temperature for LLM (0.0 = deterministic, 1.0 = creative)
# LLM_TEMPERATURE=0.3

# Number of documents to retrieve for RAG
# RETRIEVE_TOP_K=5

# ==============================================================================
# VERIFICATION
# ==============================================================================
# After setting up this file:
# 1. Rename to .env (remove .example)
# 2. Fill in your Qdrant Cloud credentials
# 3. Run: python setup.py
# 4. You should see all checks pass
#
# Required API Keys:
# - Qdrant Cloud API Key (ONLY THIS ONE!)
#
# No API Keys Needed:
# - Ollama (runs locally)
# - Sentence Transformers (runs locally)
#
# ==============================================================================